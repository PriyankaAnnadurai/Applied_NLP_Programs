{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeqwBm1jZdXG",
        "outputId": "8dc09ef1-4f11-47ba-de4e-e68dfc6bc0bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding for 'vector':\n",
            "[ 0.00180023  0.00704609  0.0029447  -0.00698085  0.00771268 -0.00598893\n",
            "  0.00899771  0.0029592  -0.00401529 -0.00468899 -0.00441672 -0.00614646\n",
            "  0.00937874 -0.0026496   0.00777244 -0.00968034  0.00210879 -0.00123361\n",
            "  0.00754423 -0.0090546   0.00743756 -0.0051058  -0.00601377 -0.00564916\n",
            " -0.00337917 -0.0034111  -0.00319566 -0.0074922   0.00070878 -0.00057607\n",
            " -0.001684    0.00375713 -0.00762019 -0.00322142  0.00515534  0.00854386\n",
            " -0.00980994  0.00719534  0.00530949 -0.0038797   0.00857616 -0.00922199\n",
            "  0.00724868  0.00536383  0.00129359 -0.00519975 -0.00417865 -0.00335678\n",
            "  0.00160829  0.0015867   0.00738824  0.00997759  0.00886734 -0.00400645\n",
            "  0.00964539 -0.00062954  0.00486543  0.00254902 -0.00062981  0.00366745\n",
            " -0.00531941 -0.00575668 -0.00760464  0.00190643  0.00652587  0.00088213\n",
            "  0.00125695  0.0031716   0.00813467 -0.00770006  0.00226075 -0.00747411\n",
            "  0.00370981  0.00951055  0.00752026  0.00642603  0.00801478  0.00655115\n",
            "  0.00685668  0.00868209 -0.00494804  0.00921295  0.0050592  -0.00213025\n",
            "  0.00848745  0.00508134  0.00964895  0.0028324   0.00986754  0.001197\n",
            "  0.00912918  0.00358697  0.00656481 -0.00361133  0.00679291  0.00724357\n",
            " -0.00213346 -0.00185955  0.00361175 -0.00703643]\n",
            "Words similar to 'vector':\n",
            "representation: 0.2467\n",
            "space: 0.1194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"Natural language processing is a field of artificial intelligence.\",\n",
        "    \"It enables computers to understand human language.\",\n",
        "    \"Word embedding is a representation of words in a dense vector space.\",\n",
        "    \"Gensim is a library for training word embeddings in Python.\",\n",
        "    \"Machine learning and deep learning techniques are widely used in NLP.\"\n",
        "]\n",
        "\n",
        "# Preprocess the text: Tokenize, remove punctuation and stopwords\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the corpus\n",
        "processed_corpus = [preprocess_text(sentence) for sentence in corpus]\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences=processed_corpus, vector_size=100, window=2, min_count=1, sg=1)  # sg=1 uses Skip-gram\n",
        "\n",
        "# Save the model for future use\n",
        "model.save(\"word2vec_model.model\")\n",
        "\n",
        "# Test the model by finding the embedding of a word\n",
        "word = \"vector\"\n",
        "if word in model.wv:\n",
        "    print(f\"Embedding for '{word}':\\n{model.wv[word]}\")\n",
        "else:\n",
        "    print(f\"'{word}' not found in vocabulary.\")\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar(word, topn=2)\n",
        "print(f\"Words similar to '{word}':\")\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(f\"{similar_word}: {similarity:.4f}\")\n"
      ]
    }
  ]
}